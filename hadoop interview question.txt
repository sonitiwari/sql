1>     Explain Hadoop Architecture?
Ans:Hadoop is an open-source framework designed to process and store large volumes of data in a distributed and fault-tolerant manner. 
 It consists of various components that work together to enable distributed processing and storage. 	

Here's an overview of the Hadoop architecture:

1>Hadoop Distributed File System (HDFS):

HDFS is the distributed file system of Hadoop. It provides high-throughput access to data across multiple machines.
Data is divided into blocks and distributed across multiple nodes in the cluster.
HDFS replicates data across multiple nodes for fault tolerance.
The NameNode manages the file system namespace and tracks the location of data blocks, while the DataNodes store the actual data blocks.

2>MapReduce:
MapReduce is a programming model and processing framework for distributed data processing in Hadoop.
It allows users to write parallelizable algorithms that process data in a distributed and fault-tolerant manner.
The processing is divided into two stages: Map and Reduce.
The Map stage processes input data and produces intermediate key-value pairs.
The Reduce stage aggregates and summarizes the intermediate results to produce the final output.

3>YARN (Yet Another Resource Negotiator):
YARN is the resource management layer of Hadoop.
It manages and allocates resources (CPU, memory) in the cluster among different applications.
YARN consists of two main components: Resource Manager and Node Manager.
The Resource Manager manages resource allocation and scheduling across the cluster.

Overall, the Hadoop architecture is designed to handle the challenges of big data processing and storage.


2>Configuration files used during hadoop installation?
During the installation and configuration of Hadoop, several configuration files are used to specify various settings and parameters. Here are some important configuration files used in Hadoop:

1.core-site.xml:This file contains configuration properties related to the Hadoop core components.
2.hdfs-site.xml:This file contains configuration properties specific to HDFS, the distributed file system component of Hadoop.
3>mapred-site.xml: This file contains configuration properties related to the MapReduce framework.
4>yarn-site.xml: This file contains configuration properties specific to YARN, the resource management component of Hadoop.
5>hadoop-env.sh: This is a shell script file used to set environment variables for the Hadoop installation.
6>hadoop-policy.xml: This file specifies the security policies for Hadoop, including access control and authorization rules.
7>log4j.properties: This file is used to configure the logging settings for Hadoop. It defines log levels, output destinations, log file formats, and other logging-related options.
8>masters and slaves: These files list the master and slave nodes in the Hadoop cluster. 



3>Difference between Hadoop fs and hdfs dfs?

hadoop fs: hadoop fs is the legacy command for interacting with the Hadoop file system. It supports multiple file systems, including HDFS and other distributed file systems compatible with Hadoop. 
it is a generic file system command that can work with different file systems based on the configuration settings.

hdfs dfs: hdfs dfs is a newer command specifically designed for interacting with HDFS. It is part of the Hadoop Distributed File System (HDFS) module and provides a focused and simplified interface for HDFS operations. It is recommended to use hdfs dfs for HDFS-specific operations.

In summary, while hadoop fs is a generic file system command that can work with various file systems, including HDFS, hdfs dfs is a focused command specifically designed for HDFS operations. It is recommended to use hdfs dfs for HDFS-specific tasks to ensure compatibility and leverage any HDFS-specific enhancements or features provided by the command.

4>Difference between Hadoop 2 and Hadoop 3?
Ans:Hadoop 3 builds upon the foundation of Hadoop 2 and introduces several important features to enhance performance, scalability, and reliability, making it a more powerful and robust big data processing platform.


YARN Resource Manager:

In Hadoop 2, the resource management and job scheduling were handled by the ResourceManager and the ApplicationMaster.
Hadoop 3 introduced the YARN (Yet Another Resource Negotiator) ResourceManager High Availability (RM HA) feature. It enables ResourceManager to run in an active-standby mode, eliminating the single point of failure and improving the reliability of the resource management system.

Hadoop 3 introduced the YARN (Yet Another Resource Negotiator) ResourceManager High Availability (RM HA) feature. It enables ResourceManager to run in an active-standby mode, eliminating the single point of failure and improving the reliability of the resource management system.

Storage Policies in HDFS:

Hadoop 3 introduced storage policies in HDFS. Storage policies allow users to specify different storage tiers for different data blocks based on their access patterns and requirements.
This feature enables organizations to use different storage media, such as Solid State Drives (SSDs) and Hard Disk Drives (HDDs), and optimize data storage based on cost and performance considerations.
This feature enables organizations to use different storage media, such as Solid State Drives (SSDs) and Hard Disk Drives (HDDs), and optimize data storage based on cost and performance considerations.

Improved NameNode Scalability:

In Hadoop 2, the scalability of the HDFS NameNode was limited due to its in-memory metadata management.
Hadoop 3 introduced the HDFS Federation feature, which allows the cluster to be scaled by adding more NameNodes for different portions of the namespace.
Additionally, Hadoop 3 introduced support for the NameNode in-memory caching to improve the overall scalability and performance.

Support for Java 8 and Beyond:

Hadoop 2 primarily supported Java 7, while Hadoop 3 added support for Java 8 and beyond. 

Other Improvements and Bug Fixes:

Hadoop 3 includes various other improvements, bug fixes, and performance optimizations over Hadoop 2.

5>      What is replication factor ? why its important
The replication factor is a configuration parameter in distributed file systems, like Hadoop Distributed File System (HDFS), that determines the number of copies of data that are stored across the cluster. It specifies how many replicas (copies) of each data block should be created and distributed to different nodes in the cluster.

Data Availability and Redundancy:
Data Durability:
Load Distribution and Parallelism:
Data Recovery and Maintenance:

6>What if Datanode fails?
By using data replication and the cooperation of the healthy Datanodes, HDFS ensures that the data remains available even when individual Datanodes fail. The cluster can continue to operate and serve data to applications without any interruption, maintaining data availability and fault tolerance.
If a Datanode fails in a Hadoop Distributed File System (HDFS) cluster, the data stored on that Datanode becomes unavailable. However, HDFS is designed to be fault-tolerant, and there are mechanisms in place to handle Datanode failures and ensure data availability.


Replication and Data Blocks:

When data is written to HDFS, it is divided into blocks, typically 128 MB or 256 MB in size. Each block is replicated multiple times across the cluster based on the configured replication factor (usually 3 by default).

Heartbeat and Block Reports:

Datanodes periodically send heartbeats to the NameNode, indicating that they are alive and functioning. The heartbeat interval is typically a few seconds.

Failure Detection:

The NameNode constantly monitors the heartbeats and block reports from all Datanodes. If a Datanode fails to send a heartbeat within a certain time, the NameNode marks it as dead.

7>What if Namenode fails?
If the NameNode fails in a Hadoop Distributed File System (HDFS) cluster, the cluster's metadata becomes unavailable, which means the HDFS cluster cannot function properly until the NameNode is restored. The NameNode is a critical component of the HDFS architecture, and its failure can have a significant impact on the cluster's availability and data accessibility.

Metadata Unavailability:

HDFS includes a component called the Secondary NameNode, which is often misunderstood as a backup NameNode. However, the Secondary NameNode's main role is to periodically download the edit logs from the primary NameNode and merge them with the fsimage (file system image) to create a new checkpoint.
If the primary NameNode fails, the Secondary NameNode cannot take over immediately because it does not have the current state of the cluster. It only has the most recent checkpoint, and to restore the state of the cluster,it needs the edit logs from the failed NameNode.


it Logs and Fsimage Backup:
To facilitate faster recovery, regular backups of edit logs and fsimage can be taken and stored securely. If the NameNode fails, these backups can be used to restore the most recent state of the cluster.
 

8>Why is block size 128 MB ? what if I increase or decrease the block size?
The default block size in Hadoop HDFS is 128 MB. This default size was chosen based on several considerations and trade-offs related to data storage, processing efficiency, and cluster performance. However, the block size is configurable, and you can increase or decrease it based on your specific use case and requirements. 

Storage Efficiency:

Larger block sizes typically lead to better storage efficiency. With larger blocks, the metadata overhead is reduced, as fewer blocks need to be managed.

Reduced NameNode Memory Usage:

A smaller block size results in more blocks and hence more metadata that needs to be stored in the NameNode's memory.

Data Processing Efficiency:

Larger block sizes can improve data processing efficiency, especially for large-scale data processing frameworks like Apache Hadoop MapReduce and Apache Spark.

9> Small file problem?
The "small file problem" is a common issue that arises in distributed file systems like Hadoop Distributed File System (HDFS) or cloud-based storage systems when dealing with a large number of small files. It refers to the negative impact of storing numerous small files on the performance, efficiency, and resource utilization of the distributed storage system.

Metadata Overhead:
Memory Usage in DataNodes:
Disk Space Fragmentation:
Data Processing Efficiency:


To address the small file problem, several strategies can be employed:

File Merging:

Periodically merge small files into larger files to reduce the number of small files and improve metadata efficiency.
SequenceFile or Parquet Format:

Use sequence files or columnar storage formats like Parquet to store small files in a more compact and efficient way.
Combine Small Files During Ingestion:

When ingesting data, try to combine small files into larger files during the data ingestion process.
Hive Partitioning:

In Hive, consider partitioning data based on specific criteria to avoid creating numerous small files.
HDFS Archives (HAR):

Use HDFS Archives to bundle small files into larger archives, reducing the metadata overhead.

By adopting these strategies, you can mitigate the small file problem and improve the overall performance and efficiency of your distributed storage system. It's important to design data ingestion and processing pipelines carefully to minimize the generation of small files and to optimize data storage and access for better performance.


10> What is Rack awareness?
Rack awareness is a concept in distributed systems, particularly in large-scale distributed storage systems like Hadoop Distributed File System (HDFS). It refers to the knowledge and organization of network topology in the cluster, specifically the physical layout of nodes (servers) in racks.

Data Replication:
Data Locality:
Network Traffic Optimization:


By leveraging rack awareness, large-scale distributed storage systems can improve data availability, optimize network traffic, and enhance overall cluster performance and fault tolerance.

11>What is SPOF ? how its resolved ?

SPOF stands for Single Point of Failure. It refers to a component or a part of a system that, if it fails, will cause the entire system to fail or become unavailable. In other words, a single point of failure is a critical component that lacks redundancy, and its failure can lead to a complete disruption of the system's operation.

Resolving or mitigating SPOFs is crucial for ensuring high availability, fault tolerance, and reliability in any system. Here are some common strategies for resolving or reducing the impact of Single Points of Failure:

redundancy
Load Balancing:
High Availability 
Monitoring and Alerting
Isolation and Resilience:
Continuous Testing and Disaster Recovery Planning:


12>what is zookeeper?
Apache ZooKeeper is an open-source server for highly reliable distributed coordination of cloud applications.ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them, which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.

Key features of ZooKeeper:
Hierarchical Namespace: 
Data Consistency and Durability: 
Watch Mechanism: 
Sequential Znodes:
Leader Election:

13>  Difference between -put and -CopyFromLocal?
In Hadoop Distributed File System (HDFS), both -put and -copyFromLocal are commands used in the Hadoop command-line interface (CLI) to copy files from the local file system to HDFS. While both commands serve the same purpose of copying files, there are some differences between them:

Command Usage:

-put: 

copyFromLocal: 

Atomicity:

-put: The -put command is atomic, meaning it copies the source file or directory to the destination as a single operation. If the copy operation fails, it does not leave any partial or incomplete data in the destination.
-copyFromLocal: The -copyFromLocal command is not atomic. It copies files one by one, and if the command fails in between, some files may have been successfully copied to the destination while others may not.
 

Compatibility:

-put: The -put command is part of the Hadoop command-line interface and is available in all Hadoop versions.
-copyFromLocal: The -copyFromLocal command is an alias for -put, and it is provided mainly for convenience. It is available in recent Hadoop versions, but older versions might not support it.

14>What is erasure coding?
Erasure coding is a method of data protection and fault tolerance used in distributed storage systems. It is an alternative to traditional data replication, where multiple copies of the same data are stored on different nodes. Erasure coding provides a more efficient way to protect data by generating and storing parity information, allowing for data recovery even if some nodes in the storage system fail.


In erasure coding, data is divided into smaller units called data blocks. Additional redundant blocks, called parity blocks

Key features of erasure coding:

Fault Tolerance: 
Storage Efficiency: 
Network Bandwidth Efficiency:
Higher Availability: 

14>  What is speculative execution?
Speculative execution is a technique used in distributed computing and parallel processing systems, particularly in frameworks like Apache Hadoop MapReduce or Apache Spark. The purpose of speculative execution is to improve job execution performance and avoid potential delays caused by slow-running tasks or nodes in the cluster.

Monitoring Task Progress:
Identifying Stragglers:
Launching Speculative Tasks:



straggler problem:In a distributed computing environment, multiple nodes or worker machines are involved in executing tasks concurrently. Each task processes a portion of the data or performs a specific computation. In some cases, due to hardware variability or data skew, certain tasks might take longer to complete compared to others. This can lead to a situation known as the "straggler problem,"


15>What is speculative execution?

Speculative execution is a performance optimization technique used in modern computer processors and multi-core systems. It is designed to improve the overall execution throughput and reduce the impact of potential delays caused by unpredictable events, such as cache misses, branch mispredictions, or long-latency operations.

In a speculative execution model, the processor attempts to predict the outcome of certain instructions or branches and starts executing instructions based on those predictions before the actual result is known.

16.   Explain Yarn Architecture?

User

16.   Explain Yarn Architecture


YARN (Yet Another Resource Negotiator) is the resource management layer in Apache Hadoop. It is responsible for managing and allocating resources in a Hadoop cluster and serves as the distributed operating system for big data applications. YARN was introduced in Hadoop 2.x to improve scalability and resource utilization in large clusters, replacing the older MapReduce-specific resource manager.

YARN architecture is divided into two main components:

Resource Manager (RM):

The Resource Manager is the central component of YARN and acts as the master node in the cluster. It is responsible for overall resource management, allocation, and scheduling across the cluster.
The Resource Manager has two main components: the Scheduler and the Application Manager.
Scheduler: The Scheduler is responsible for allocating resources to different applications or jobs running in the cluster. It maintains a queue of pending applications and assigns resources to them based on configured policies (FIFO, Capacity, Fair Scheduler, etc.).
Application Manager: The Application Manager is responsible for managing the life cycle of individual applications. When a new application is submitted to the cluster, the Application Manager negotiates resources from the Scheduler and launches the Application Master for that application.
Node Manager (NM):

The Node Manager runs on each node (worker machine) in the Hadoop cluster and is responsible for managing the resources (CPU, memory, disk, etc.) available on that node.
The Node Manager communicates with the Resource Manager to advertise the available resources and get instructions for running containers.
Container: A container is a fundamental unit of resource allocation in YARN. It represents a specific allocation of resources (CPU and memory) on a node where an application's tasks will run.
The Node Manager launches and monitors containers on each node as instructed by the Resource Manager


17>.   How does ApplicationManager and Application Master  differ?

In Apache Hadoop YARN, the terms "ApplicationManager" and "Application Master" refer to two different components responsible for managing applications running in the cluster. While they are related, they serve different purposes and operate at different levels within the YARN architecture.

Application Manager:

The Application Manager is a component of the YARN Resource Manager (RM). It is responsible for managing the life cycle of applications in the YARN cluster.
When a new application is submitted to YARN, the Application Manager is the first point of contact. It performs the initial negotiation with the Resource Manager to allocate resources for the application.

Application Master:

The Application Master is a per-application component and runs on a cluster node allocated to that specific application. It is responsible for managing the execution of tasks belonging to that application.

18>Explain Mapreduce working?

MapReduce is a programming model and processing paradigm used in Apache Hadoop and other distributed data processing frameworks. It is designed to process large-scale data sets in parallel across a cluster of computers, enabling efficient and scalable data processing. The MapReduce model divides the data processing tasks into two main phases: the Map phase and the Reduce phase.



MapReduce is a programming model and processing paradigm used in Apache Hadoop and other distributed data processing frameworks. It is designed to process large-scale data sets in parallel across a cluster of computers, enabling efficient and scalable data processing. The MapReduce model divides the data processing tasks into two main phases: the Map phase and the Reduce phase.

Here's how MapReduce works:

Input Data Splits:

The input data is first divided into smaller manageable chunks called "input splits." Each input split represents a portion of the overall data that can be processed independently.
Map Phase:

In the Map phase, the processing tasks are divided into multiple parallel instances called "mappers."
Each mapper takes an input split and applies a user-defined "map" function to process the data.
The map function transforms the input data into a set of key-value pairs as intermediate output.
The intermediate key-value pairs are not written to disk immediately but are held in memory temporarily or spilled to disk if necessary.
Shuffling and Sorting:

The intermediate key-value pairs from all mappers are grouped based on their keys and sorted. This process is known as "shuffling and sorting."
The purpose of shuffling and sorting is to bring together all the values corresponding to the same key from different mappers, ensuring that they are processed together during the Reduce phase.
Reduce Phase:

In the Reduce phase, the processing tasks are divided into multiple parallel instances called "reducers."
Each reducer takes a group of key-value pairs with the same key and applies a user-defined "reduce" function to process the data.
The reduce function typically aggregates or combines the values for each key, producing a set of output key-value pairs.
Final Output:

The final output of the MapReduce job is the result of all the reducers' outputs, which are written to the output data store, such as Hadoop Distributed File System (HDFS).


19>   How many mappers are created for 1 GB file?
 
Number of Mappers = (Size of File) / (Block Size)
= 1 GB / 128 MB
≈ 8

20.   How many reducers are created for 1 GB file?

Number of Reducers = (Total Input Data Size) / (Estimated Reducer Output Size)
= 1,073,741,824 bytes / 128 MB
= 8.38 (approx.)



21> What is combiner?

In the context of Hadoop MapReduce and other distributed data processing frameworks, a Combiner is a special type of function used to perform a local aggregation of intermediate key-value pairs on each individual mapper node before the data is sent to the reducers. The Combiner helps reduce the volume of data transferred over the network during the shuffle and sort phase, which can lead to significant performance improvements.

Here's how the Combiner works in the MapReduce process:

Map Phase:

During the Map phase, the mappers process the input data and produce intermediate key-value pairs as the output.
These intermediate key-value pairs are grouped by key and sorted to prepare them for the shuffle and sort phase.
Combiner Execution:

Before the shuffle and sort phase, the Combiner function is applied to the output of each mapper locally on the same node where the mapping occurred.
The Combiner aggregates the values associated with each key, typically by performing some kind of summing, averaging, or other reduction operation.
The purpose of the Combiner is to reduce the amount of data that needs to be transferred over the network during the shuffle phase, as the combined values are smaller than the original key-value pairs.
Shuffle and Sort Phase:

After the Combiner has run locally on each mapper node, the intermediate key-value pairs are shuffled and sorted based on their keys.
The sorted intermediate data is then sent to the appropriate reducer nodes for further processing.
The Combiner's functionality is similar to that of the Reducer, but with an essential difference: the Combiner runs locally on the mapper nodes, while the Reducer runs on the reducer nodes. The Combiner's output is not the final result of the job; it only provides a local reduction to reduce the data volume that needs to be transferred across the network.


22> What is partitioner?
In Hadoop MapReduce and other distributed data processing frameworks, a Partitioner is a component responsible for assigning intermediate key-value pairs generated by the mappers to specific reducers. The purpose of the Partitioner is to ensure that all the key-value pairs with the same key end up in the same reducer, so they can be processed together during the reduce phase.

Here's how the Partitioner works in the MapReduce process:

Map Phase:	
During the Map phase, the mappers process the input data and produce intermediate key-value pairs as the output.
These intermediate key-value pairs are grouped by key and sorted in preparation for the shuffle and sort phase.

Partitioning:

After the shuffle and sort phase, the Partitioner comes into play. Its task is to take the sorted intermediate key-value pairs and decide which reducer each pair should be sent to for further processing.
The Partitioner uses the key of each intermediate pair to make this decision.



